{"podcast_details": {"podcast_title": "AI Today Podcast: Artificial Intelligence Insights, Experts, and Opinion", "episode_title": "AI Today Podcast: Trustworthy AI Series: Ethical AI", "episode_image": "https://www.aidatatoday.com/wp-content/uploads/2023/02/New-AI-Today-Podcast-Image-1.png", "episode_transcript": " The AI Today podcast produced by Cognolitica cuts through the hype and noise to identify what is really happening now in the world of artificial intelligence. Learn about emerging AI trends, technologies and use cases from Cognolitica analysts and guest experts. Hello and welcome to the AI Today podcast. I'm your host Kathleen Walsh. And I'm your host Ron Schmelzer. And you know, thank you again. As you know, we've been well on our way with the Glossary Series podcast. We've had a number of great interviews as well with some great luminaries in the AI space. And we're just getting started with our trustworthy AI series podcast. We've had a couple of podcast episodes so far. We have some more queued up and we wanted to vary it up a little bit. I think some people we hear from you. You love our Glossary Series. So a lot of you are really especially as you're going to some of these more esoteric concepts that are but they do explain what's going on. So it's very helpful. Some of you who are getting our training, our CPMI certification, other of you who are just learning these terminology so you can be better informed and have a good conversation. Well, but we also have heard from some of you that, you know, maybe it's time for a little change of pace. So we are trying to intersperse the glossary with more topics. And, of course, this one we're focusing more on some of the trustworthy stuff because making AI work in today's reality means paying attention to some of these really big issues that will kill your projects if you don't do it the right way. Exactly. And so we thought it was important to bring this trustworthy AI series because many of the companies that we work with, we've started to talk to about this more. And I think in general with the news with everything that's going on. Yes, AI is, you know, seems to be in the news, a lot lately and people are using it more in their everyday lives, which is wonderful, but in order to make sure that these systems really do provide the benefit that they seek right at the end of the day. That's what we're hoping for. We want to make sure that we are, we are building trustworthy AI systems and in order to do that. That means that, yes, you need to follow, you know, frameworks, learn from others and actually understand what it means. So that's why we thought it was important to bring this series. If you haven't already, make sure to subscribe to AI today. We have a lot more in this series coming up and also go back and look at some of the past episodes that we have on the trustworthy AI series. We'll link to that in the show notes as well just so that you can see them all. So on today's podcast, we want to talk about the ethical AI layer of the trustworthy AI framework and we think it's important to dive deeper into each of these levels. So in one podcast, which will link to we had given an overview of all of them, but we want to dig deeper into the ethical AI layer and say, you know, why are societal ethics important for AI systems and what does that mean? Yeah, as we mentioned, there's five main layers of trustworthy AI, which encapsulates a lot of different ideas. And the reason why they're in layers is because they're not really all addressing things from that same perspective. So as Kathleen mentioned, we'll link to the podcast where we talk about those five layers. But to refresh your memory, this ethical layer really deals with issues of AI systems in society and trying not to do things that would hurt people or hurt society. And so that generally covers a bunch of ideas that we want not only systems to have, but also people, right? We want machines that will comply with the most fundamental of our human values, you know, doing no harm, right? Things like that. We don't also don't want to build systems that make whatever ethical problems we have in the world. We don't want to make them worse. We don't want to make machines that basically are the dystopian version of humanity, take whatever ills we have in society and just magnify them. Certainly, we've seen a lot of science fiction movies and when you watch the dystopian ones, that's what happens. You know, you take things to the extreme. We don't want to make systems that do that. We also want to avoid not only physical harm, that is, you know, machines that cause actual physical harm to people, but other forms of harm, you know, emotional harm, financial harm, you know, any sort of harm to their reputation, harm to their ability to get things done and live their lives. You know, we don't want to be building systems that do that, right? And we also want to make sure that whatever AI systems we build will never be beyond our ability to control them. Because if we can't control them, then of course we can't make sure any of these ethical systems, any of these ethical principles will happen. Because if we can't control them anymore, then systems will make decisions for whatever systems will do, ethics be gone. So at this layer, we talk about these big societal ethical issues. Exactly. And so we say, you know, do no harm. But what does that mean? We say to make AI systems trustworthy and beneficial because yes, we want, you know, to build trustworthy systems, but we also want to make sure that they are, you know, beneficial for a purpose. That's actually doing something. We always say don't do AI just for AI sake. So if we are going to, you know, do no harm, we need to make sure that we have systems in place that can do things like, you know, we said don't cause harm. And so what does harm mean? Well, harm can be physical harm. So we definitely don't want to injure humans because of this. But we also don't want financial harm, emotional harm, and that's something that not everybody addresses, but is incredibly real. I talked in a previous podcast about fears and concerns related to AI. You know, some of these are emotional. We want to make sure that we're not doing any emotional harm. Also societal harm, environmental harm as well. A lot of people don't talk about that. And that is in this layer where we don't want to be having a negative impact on the environment for, you know, humans. We don't want to do that. And so we don't want machines to do that either. And also mental harm. So these, you know, that sort of goes with emotional, but really we want to make sure that these are beneficial. And so we're trying not to have harm. We also want to make sure that systems aren't being misused or abused. This is incredibly important and part of that trustworthy AI, you know, especially with the societal ethics because it can be easy for people sometimes to misuse and abuse technology. And again, when we had talked about, you know, we don't want this to be beyond human control. AI systems shouldn't be beyond the ability for humans to control their actions. A lot of people, especially if you have, you know, more philosophical debates on this, we talk about, you know, what happens and how do you actually control these machines? And we say, this isn't something that you can just unplug. There was a, you know, people talk about you can't unplug the Internet anymore. So it's out there. And how do you go about living your life with the Internet and the things that are on the Internet, whether or not you necessarily wanted them on there or intended them to be on there, you know, maybe 10, 15, 20 years later now at this point. AI systems also should provide some element of transparency and explainability. Yes, there are, you know, certain algorithms that are going to be more explainable than others. But at the end of the day, you need to be thinking about when we're saying do no harm. What what level and what element of transparency and explainability do we need in these systems? And how do we go about thinking about that? So really trustworthy AI is going to require ethical AI. And that should hopefully by now you're getting it. You're saying, OK, what do I need to be thinking about when I'm thinking about ethical AI when I'm thinking about doing no harm? How do I go about doing that? Yeah, you know, and some of it, as Kathleen mentioned, are these fears. And and certainly we can build systems that, well, cause people to be scared because people are scared about things like general intelligence, even though we don't haven't yet built any system that is capable of all the general intelligence tasks that humans are capable of. That's why we call artificial general intelligence, because we don't need to have one system that does image recognition, another system that does in text to image, another system that does something else. What if we can build that one superhuman, you know, superintelligent system that can do all these things? That's what people are scared of, because, well, machines can do things that humans can't. And if you give them the intelligence that humans have, then I think we're at a little bit of a disadvantage. So there's a lot of this concern about AGI more so than there is concern about the narrow applications, even though we don't have any AGI, we only have narrow applications. Therefore, we really should be more scared of the narrow ones because that's what we have today. But people are really worried about that. They're worried about the singularity, which is this idea that at some point machines will learn how to learn and then we will never be able to catch up. You know, there's these popularized ideas in various books like, you know, Life 3.0 or Superintelligence or AI or Final Invention, where they talk about the paperclip optimizer, which are AI systems that are so narrowly focused on a goal that they destroy everything else. It's such a tiny, tiny risk for a while. You know, people in the press, like notable personalities like Elon Musk and Hawking and Bill Gates were like so concerned about this that they were like, hey, guys, we got to be really worried about it. But then again, now they're building systems that are actually doing some of those bad things, which kind of makes you wonder. So really what we have to do with ethical AI, some of this has to do with just laws. But, you know, we can't count on laws because different countries have different concerns and people will always go to where things are possible. So this really has to do with putting in like logical, common sense things in our applications so that we're not intentionally building the robots of destruction, even though we haven't built AGI yet. And we tell people have a little bit of a reality check. You'll know that we're kind of heading towards Superintelligence when you can talk to your voice assistants and they're not dumb because still you talk to your voice assistants, you tell them something basic and they don't understand it. So, you know, we're really not anywhere near that, but we have to build systems that don't prey on people's fears. Exactly. I think that's pretty important. Whenever we talk about artificial intelligence and honestly transformative technology of any kind, people are always concerned about workforce disruption and how that's going to impact their jobs, both on a, you know, micro and macro level. So the question always gets asked, will AI systems replace human workers or will AI systems replace certain types of workers certain jobs roles and functions. And again, as we said, this is, you know, any form of technology that we bring in, people are always going to be concerned about this for a long time we have been saying that AI is not a job killer but it is a job category killer so classes of some jobs will most likely disappear, but new jobs will be introduced and so if we think about this with any sort of transformative technology that's been the case. When desktop computers came out, we didn't need to have rooms of secretaries anymore but that didn't mean that we had mass unemployment. We've also had new jobs created, you know, back in the 1960s social media marketer job absolutely did not exist social media did not exist. And we have many jobs in that category in general, and this is only going to continue to happen. So yes, it may displace some jobs some roles, but in general, it's not going to cause mass disruption. And we're starting to bring up especially in this, you know, societal ethics, because certain companies or certain countries are very concerned about this. So we always say, keep the human in the loop, make sure that you're not building systems that are meant to replace humans and please don't you know build systems that are meant to create mass unemployment, because even if jobs do eventually catch up that can be quite disruptive. We said don't do emotional financial harm that's definitely going to be a mental harm, all of that so don't try and do that but just understand that understand that, you know, we talked about those fears and concerns. This is something that people will continue to bring up and something that you're going to need to continue to address, whether that's within your industry that's within your organization, your group, however that is. Obviously, machines will replace, you know, people's jobs individuals jobs so you can say don't build machines that won't replace jobs and well that's sort of the purpose of a machine is to replace a job but what we really mean is don't build AI systems whose intent is to destroy whole job categories that is a much more bigger consideration because you have to think about the out the outside of what of what happens there and that's the sort of a trickier one and you. So you're saying words like well how do you know if that's the kind of situation you're doing with how do you know if that's the kind of AI system you're building is one that has the intent of mass job unemployment. Well you kind of sometimes know when that happens right. You know even even in the non AI system when you build things like Airbnb or Uber you know when you when you know you're disrupting an industry with when that's the intent, we are going to disrupt the hotel industry we're going to disrupt the taxi industry, you know that the people who work in the hotel industry or the taxi industry will probably have some impact their jobs now turns out that it's probably impacted taxes a lot more than hotels it's a whole other story but this is how you kind of know when you're building an AI system that's going to do that it's pretty obvious from the get go. So when we talk about this ethical layer turns out there's a lot of different things we talked about just a few of them we just talked about do no harm we talked about, you know, AI systems out of human control we talked about your job replacement but there's a lot of things that are wrapped into this layer of trustworthy AI there's this whole idea of we shouldn't have build machine systems that exhibit characteristics that they should exhibit the characteristics that humans exhibit they should have the human values and whatever we value as humans, machine should have that same value if they don't value our lives or they don't value our privacy if they don't value our finances they don't value our emotional state then we know we're having a problem right related to that but differently is this idea of dignity which is that we shouldn't have machines that treat humans in a subhuman way, which actually is starting to happen where people are made to do things, act like machines, you know, submit to the machine you know that's the ultimate vision of this if this was like some sort of science fiction movie, it would be a machine that would say bow down to me, and you tell me that I'm the greatest thing alive for that that would be kind of a science fiction II, but there are equivalents of this where we've had systems where they make you do things because the machine wants you to do it not because a human would do it like repeat things over and over again, say things in different ways, or make you feel bad so you need to think about when you're building AI systems that sort of denigrate the human experience and how do you make sure not to do it related to that are these issues of fairness we shouldn't build AI systems that favor one group or one type over another. And related to that is we should build AI systems that are accessible to the broadest state of humanity. This is the diversity and inclusion, not from the perspective of, we just want to make the organizations be more representative that's important to, but we don't want to have AI systems that are exclude people or only incorporate data for a certain group that becomes more harmful than good rather than just altruistic. We don't want to actually cause real harm related to that. We don't want to have systems that have explicit biases or explicit discrimination we shouldn't be building AI systems for that. On the on the related note is this idea of that we want to give people still freedom of choice. There's this idea of freedom and agency we shouldn't have an AI system whose particular purpose is to limit your choice to basically say no, you are not going to go to this destination you are not going to use this bank account you're not going to buy these products you're not going to watch these movies. So that's when we're starting to get into some emotional harm right. What if you logged into Netflix one day it's like now I'm not going to show you those movies and you click to watch you like nope denied. What, what kind of system is that right. We're not that far away from actually doing it so we should not do it. Right. And then that kind of relates to this idea of human benefit, we should really have AI systems that are built for the benefit of humanity, built for the benefit of just a few people or some system. And it goes to share. It goes without saying that we need these AI systems to operate within our control and never operate in some sort of autonomous fashion that takes away all of our ability to impact anything. It's very frustrating to like have an interaction with say, Google or YouTube and then all of a sudden you're denied, or maybe your account is shut off, and you have nobody to talk to there's like no recourse, other than talking to machine. So this is not just AI systems like the Terminator out of control these are even basic narrow AI systems where there's no human control. Literally there's no one to talk to, and you're out of luck. So that's that's a really big issue. And going along with this is we should not have AI systems that just who either intentionally or unintentionally destroy the environment that would not be so great for humanity either. Exactly. That would be terrible. So it's one thing to understand what needs to be addressed in the societal ethical layer, but then how do you actually implement this practically because at the end of the day, that's the most important thing right it's one thing to talk about it but it's another thing to actually implement it. So when you're looking at societal ethics, there should be some questions that you ask, no matter what industry you're in no matter what size your organization is. And these include things like what basic requirements for human values, will you require for all your projects, and what will you require from technology vendors and service providers. So, if you're working with others, what are you going to require outside parties to do that you're working with, and what will you communicate and require internally and what will be externally communicated so you may have one set of, you know, internal communication that all employees and people that you work with must abide by and understand, and then maybe externally you want to present that to others. It's important to have this and the thing that's really important is that you're bringing together this diverse group internally as well right you know we said part of this is you want to make sure that you know diversity and inclusion is part of societal ethics and yes that's externally but you have to look at that internally as well. Sometimes people can just get so focused on projects and you know maybe they bring in people that are enthusiastic or people that they feel would be interested in this but it is important to bring in that diverse group from all different departments, and that's something that we have helped many organizations do with our trustworthy AI workshop, because you need to make sure that you're bringing in all those different parties. So when you're doing that, you need to make sure then that you are addressing those considerations, all of the things that we talked about right human values, dignity, fairness, diversity and inclusion, bias and discrimination, freedom and agency, human benefit, human control and respect for the environment. These can be project specific, you can also depending on what it is you know, have different considerations, but it is really important that you are going through this and considering it, none of them should be skipped. They all should be addressed and this takes time. And this is why it's helpful to have a group help you know come and gather so that you are really addressing all of these topics, asking the questions and some of these are not going to be easy questions to ask or answer, but they all need to make sure that you check all of them. And you know, it was in this context that we put together a while ago a framework which is basically a, we looked at over 60 different ethical and responsible AI frameworks and guidelines and they call them all sorts of things, because we wanted to see what people were doing and we're like well surely somebody has put together some you know comprehensive list of all the things we need to worry about and give us some guidance as to well how do I, it's good to worry about these things but it's probably more practical to do something about it right? Or to tell me what I should do about it or how I should do it to keep myself out of trouble, to keep myself in complying with the laws that are out there, you know reduce the risk of lawsuits, it's becoming more and more and when we do these courses we actually talk about ripped from the headlines where we don't just talk about a theory, we talk about this company was sued, this person got hurt, this happened, this organization lost the reputation, this company ended up having to kill their projects, oh there's so many examples we barely even have to scratch the surface to find dozens if not hundreds of them and you don't want to be one of them, do you? I mean you want a successful AI project, why would you want to run something in a purposely unethical way right? So when we put together this framework, well first of all we thought there was an existing framework, to our surprise and maybe disappointment there really isn't one, there isn't one framework out there, there wasn't one at least, a framework out there that had the whole range. So we compiled it all together and we put it together and we give it away under creative comments so you can use it. So that's our Trustworthy AI framework which is downloadable, if you go again to cognitlitica.com slash trustworthy, that's sort of our landing page for all this stuff, the intro to free course, the workshop, our additional content and downloadables, if you go there, you'll find this framework, and it really helps you with all these sorts of things. But I think we realized that organizations, especially larger ones, have a hard time actually putting together the practical steps that they need to do in their current projects as well as future projects, as well as how they interact with their internal organizations, legal, risk, compliance, privacy, the data folks, the security folks, there's a lot of folks as Kathleen mentioned, the data people, the data stewards, the data organization, and also how they tell other people, whether it's their customers, their partners, their employees, their suppliers, it's a very, very difficult task. And we find a lot of people just try to do it all themselves and they're like chickens without a head, they're running around, they're missing things, they're doing things in a dumb way, I mean I have to be totally honest, a lot of dumb stuff. So what we've done is we've like, look, you know, for those organizations who really truly want to do something about their systems, make them trustworthy, not run around like chickens without a head, we actually have a three-day in-person on-site workshop. We, one, we align your knowledge and understanding of all these various considerations for ethical and responsible AI to make them trustworthy. Two, we develop, we all collaboratively during these three days develop an actual, practical, implementable, organization-specific, comprehensive, trustworthy AI framework that for each one of these considerations talks about what you're going to do about it now, what you'll do in the future, who is involved, how you do it, the various rules and processes and guidelines to do it. And for things that you don't know what you're doing, you can at least say, hey, we don't have an answer for this, that's perfectly okay. But then maybe you iterate and later you have an answer for it. And that brings you to a checklist that we also developed during these three days that provides guidance on what do you do for the developers, how do you deal with the data folks, how do you deal with privacy, legal, a checklist for every step in the project to make sure that you're not unintentionally or intentionally causing all sorts of harms or other problems that will cause issues for you from a liability risk perspective. And of course, finally, for your organizations, what are the controls, practices and procedures, the audits, the regulations that you will put into place whenever you build, deploy, manage, monitor or change a system? So this is not for all of you. This is all for those of you that are really sort of struggling to make this work. We understand it's a subset of our audience. But for those of you that really want to do it, I mean, this is this is the fast way to get started. And we really encourage you again to go to Cognoletica.com slash trustworthy to check out this workshop if it makes sense for you. And for all others, we have other course material, our course, our intro to trustworthy course. We have our downloadable framework you could take a look at and some other content we prepared for you to help you on your journey. Exactly. So we'll make sure to link to that in the show notes like we always do. And if you haven't heard about our free intro to trustworthy course or you're interested in learning more, it's about two hours and it'll help bring you into the fundamental concepts of trustworthy. So it'll just dig a little bit deeper into the content that we share on the podcast over our series of this trustworthy podcast series. Like we said, if you want to learn more, go to Cognoletica.com slash trustworthy. We'll link to that in the show notes as well. It's a free intro course. It'll help if you're debating if this is right for your organization. We encourage you to just check it out and see if it is for you. With that, we will you know, we do love to hear from our listeners. And as Ron mentioned at the beginning of the podcast, we really do take what you say into consideration. It helps guide some of the additional topics that we discuss on the podcast. So if you have anything that you're interested in us digging deeper on, absolutely reach out. You can find us on LinkedIn or on Cognoletica.com or go to the podcast and rate us. And you can also share what you like about the podcast there. So rate us on Apple Podcasts, Google, Spotify, or your favorite podcast platform. Like this episode and want to hear more? With hundreds of episodes and over 3 million downloads, check out more AI Today podcasts at AItoday.live. Make sure to subscribe to AI Today if you haven't already on Apple Podcasts, Spotify, Stitcher, Google, Amazon, or your favorite podcast platform. Want to dive deeper and get resources to drive your AI efforts further? We've put together a carefully curated collection of resources and tools, handcrafted for you, our listeners, to expand your knowledge, dive deeper into the world of AI, and provide you with the essential resources you need. Check it out at AItoday.live.list. This sound recording and its contents are copyright by Cognoletica, all rights reserved. Music by Matsu Gravas. As always, thanks for listening to AI Today and we'll catch you at the next podcast."}, "podcast_summary": "The AI Today podcast, produced by Cognolitica, delves into the world of artificial intelligence, separating fact from hype. The podcast covers emerging trends, technologies, and use cases, with insights from Cognolitica analysts and guest experts. In this episode, the hosts Kathleen Walsh and Ron Schmelzer discuss the importance of ethical AI within the larger framework of trustworthy AI. They highlight the need to build AI systems that comply with human values and do no harm, avoiding physical, emotional, financial, reputational, and environmental harm. They emphasize the importance of human control over AI systems and the need for transparency and explainability. The hosts also touch on the fears and concerns surrounding AI, such as job displacement and the potential dangers of superintelligence. They stress the importance of considering societal ethics and implementing practical steps to ensure ethical and responsible AI. The hosts provide resources, including their Trustworthy AI framework, to guide organizations in building and maintaining trustworthy AI systems. They also invite listeners to share their feedback and suggestions for future podcast topics.", "podcast_guest": {"podcast_guest": {"name": "Elon Musk", "summary": "Elon Musk's Tesla Roadster is an electric sports car that served as the dummy payload for the February 2018 Falcon Heavy test flight and became an artificial satellite of the Sun. A mannequin in a spacesuit, dubbed \"Starman\", occupies the driver's seat."}}, "podcast_highlights": "1. \"We shouldn't have build machine systems that exhibit characteristics that they should exhibit the characteristics that humans exhibit; they should have the human values and whatever we value as humans, machines should have that same value if they don't value our lives or they don't value our privacy if they don't value our finances they don't value our emotional state then we know we're having a problem right related to that but differently is this idea of dignity which is that we shouldn't have machines that treat humans in a subhuman way, which actually is starting to happen where people are made to do things, act like machines, you know, submit to the machine you know that's the ultimate vision of this if this was like some sort of science fiction movie, it would be a machine that would say bow down to me, and you tell me that I'm the greatest thing alive for that that would be kind of a science fiction II, but there are equivalents of this where we've had systems where they make you do things because the machine wants you to do it not because a human would do it like repeat things over and over again, say things in different ways, or make you feel bad so you need to think about when you're building AI systems that sort of denigrate the human experience and how do you make sure not to do it.\"\n2. \"There's a lot of folks as Kathleen mentioned, the data people, the data stewards, the data organization, and also how they tell other people, whether it's their customers, their partners, their employees, their suppliers, it's a very, very difficult task. And we find a lot of people just try to do it all themselves and they're like chickens without a head, they're running around, they're missing things, they're doing things in a dumb way, I mean I have to be totally honest, a lot of dumb stuff. So what we've done is we've like, look, you know, for those organizations who really truly want to do something about their systems, make them trustworthy, not run around like chickens without a head, we actually have a three-day in-person on-site workshop.\"\n3. \"So we compiled it all together and we put it together and we give it away under creative comments so you can use it. So that's our Trustworthy AI framework which is downloadable, if you go again to cognoletica.com slash trustworthy, that's sort of our landing page for all this stuff, the intro to free course, the workshop, our additional content and downloadables, if you go there, you'll find this framework, and it really helps you with all these sorts of things.\""}